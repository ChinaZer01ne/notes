# 全文检索



## Lucene

### 入门





## Solr

### 原理

它可以归结为两个过程：**1.索引创建（Indexing）2. 搜索索引（Search）**。那么索引到底是如何创建的呢？索引里面存放的又是什么东西呢？搜索的的时候又是如何去查找索引的呢？

![img](/images/全文检索/solr原理.png)



### 索引

Solr/Lucene采用的是一种反向索引，所谓**反向索引**：就是从关键字到文档的映射过程，保存这种映射这种信息的索引称为反向索引（通过关键字来查找文档）

![inverted_index_thumb.jpg](/images/全文检索/solr倒排表.png)

- 左边保存的是字符串序列
- 右边是字符串的文档（Document）编号链表，称为倒排表（Posting List）

字段串列表和文档编号链表两者构成了一个字典。现在想搜索”lucene”，那么索引直接告诉我们，包含有”lucene”的文档有：2，3，10，35，92，而无需在整个文档库中逐个查找。如果是想搜既包含”lucene”又包含”solr”的文档，那么与之对应的两个倒排表去交集即可获得：3、10、35、92。

### 索引创建

假设有如下两个原始文档：
文档一：Students should be allowed to go out with their friends, but not allowed to drink beer.
文档二：My friend Jerry went to school to see his students but found them drunk which is not allowed.
创建过程大概分为如下步骤：
![index-build](/images/全文检索/solr索引创建流程.png)

####一：把原始文档交给分词组件(Tokenizer)

分词组件(Tokenizer)会做以下几件事情(这个过程称为：Tokenize)，处理得到的结果是词汇单元（Token）

1. **将文档分成一个一个单独的单词**

2. **去除标点符号**

3. **去除停词(stop word)**

   - 所谓停词(Stop word)就是一种语言中没有具体含义，因而大多数情况下不会作为搜索的关键词，这样一来创建索引时能减少索引的大小。英语中停词(Stop word)如：”the”、”a”、”this”，中文有：”的，得”等。不同语种的分词组件(Tokenizer)，都有自己的停词(stop word)集合。经过分词(Tokenizer)后得到的结果称为词汇单元(Token)。上例子中，便得到以下

     词汇单元(Token)

     ：

     ```
     "Students"，"allowed"，"go"，"their"，"friends"，"allowed"，"drink"，"beer"，"My"，"friend"，"Jerry"，"went"，"school"，"see"，"his"，"students"，"found"，"them"，"drunk"，"allowed"
     ```

####二：词汇单元(Token)传给语言处理组件(Linguistic Processor)

语言处理组件(linguistic processor)主要是对得到的词元(Token)做一些语言相关的处理。对于英语，语言处理组件(Linguistic Processor)一般做以下几点：

1. **变为小写(Lowercase)。**
2. **将单词缩减为词根形式，如”cars”到”car”等。这种操作称为：stemming。**
3. **将单词转变为词根形式，如”drove”到”drive”等。这种操作称为：lemmatization。**

语言处理组件(linguistic processor)处理得到的结果称为**词(Term)**，例子中经过语言处理后得到的词(Term)如下：

```
"student"，"allow"，"go"，"their"，"friend"，"allow"，"drink"，"beer"，"my"，"friend"，"jerry"，"go"，"school"，"see"，"his"，"student"，"find"，"them"，"drink"，"allow"。
```

经过语言处理后，搜索drive时drove也能被搜索出来。**Stemming 和 lemmatization的异同：**

- 相同之处：
  1. Stemming和lemmatization都要使词汇成为词根形式。
- 两者的方式不同：
  1. Stemming采用的是”缩减”的方式：”cars”到”car”，”driving”到”drive”。
  2. Lemmatization采用的是”转变”的方式：”drove”到”drove”，”driving”到”drive”。
- 两者的算法不同：
  1. Stemming主要是采取某种固定的算法来做这种缩减，如去除”s”，去除”ing”加”e”，将”ational”变为”ate”，将”tional”变为”tion”。
  2. Lemmatization主要是采用事先约定的格式保存某种字典中。比如字典中有”driving”到”drive”，”drove”到”drive”，”am, is, are”到”be”的映射，做转变时，按照字典中约定的方式转换就可以了。
  3. Stemming和lemmatization不是互斥关系，是有交集的，有的词利用这两种方式都能达到相同的转换。

####三：得到的词(Term)传递给索引组件(Indexer)

1. 利用得到的词(Term)创建一个字典

   ```
   Term    Document ID
   student     1
   allow       1
   go          1
   their       1
   friend      1
   allow       1
   drink       1
   beer        1
   my          2
   friend      2
   jerry       2
   go          2
   school      2
   see         2
   his         2
   student     2
   find        2
   them        2
   drink       2
   allow       2
   ```

2. 对字典按字母顺序排序：

   ```
   Term    Document ID
   allow       1
   allow       1
   allow       2
   beer        1
   drink       1
   drink       2
   find        2
   friend      1
   friend      2
   go          1
   go          2
   his         2
   jerry       2
   my          2
   school      2
   see         2
   student     1
   student     2
   their       1
   them        2
   ```

3. 合并相同的词(Term)成为文档倒排(Posting List)链表



   - Document Frequency：文档频次，表示多少文档出现过此词(Term)
   - Frequency：词频，表示某个文档中该词(Term)出现过几次

对词(Term) “allow”来讲，总共有两篇文档包含此词(Term)，词（Term)后面的文档链表总共有两个，第一个表示包含”allow”的第一篇文档，即1号文档，此文档中，”allow”出现了2次，第二个表示包含”allow”的第二个文档，是2号文档，此文档中，”allow”出现了1次

至此索引创建完成，搜索”drive”时，”driving”，”drove”，”driven”也能够被搜到。因为在索引中，”driving”，”drove”，”driven”都会经过语言处理而变成”drive”，在搜索时，如果您输入”driving”，输入的查询语句同样经过分词组件和语言处理组件处理的步骤，变为查询”drive”，从而可以搜索到想要的文档。

### 搜索步骤

搜索”microsoft job”，用户的目的是希望在微软找一份工作，如果搜出来的结果是:”Microsoft does a good job at software industry…”，这就与用户的期望偏离太远了。如何进行合理有效的搜索，搜索出用户最想要得结果呢？搜索主要有如下步骤：

**一：对查询内容进行词法分析、语法分析、语言处理**

1. 词法分析：区分查询内容中单词和关键字，比如：english and janpan，”and”就是关键字，”english”和”janpan”是普通单词。
2. 根据查询语法的语法规则形成一棵树
   ![grammer_tree.jpg](/images/全文检索/solr搜索.png)
3. 语言处理，和创建索引时处理方式是一样的。比如：leaned–>lean，driven–>drive

**二：搜索索引，得到符合语法树的文档集合**
**三：根据查询语句与文档的相关性，对结果进行排序**

我们把查询语句也看作是一个文档，对文档与文档之间的相关性（relevance）进行打分（scoring），分数高比较越相关，排名就越靠前。当然还可以人工影响打分，比如百度搜索，就不一定完全按照相关性来排名的。

如何评判文档之间的相关性？一个文档由多个（或者一个）词（Term）组成，比如：”solr”， “toturial”，不同的词可能重要性不一样，比如solr就比toturial重要，如果一个文档出现了10次toturial，但只出现了一次solr，而另一文档solr出现了4次，toturial出现一次，那么后者很有可能就是我们想要的搜的结果。这就引申出权重（Term weight）的概念。

**权重**表示该词在文档中的重要程度，越重要的词当然权重越高，因此在计算文档相关性时[影响力](http://www.amazon.cn/gp/product/B0044KME2E/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&tag=importnew-23&linkCode=as2&camp=536&creative=3200&creativeASIN=B0044KME2E)就更大。通过词之间的权重得到文档相关性的过程叫做**空间向量模型算法(Vector Space Model)**

影响一个词在文档中的重要性主要有两个方面：

- Term Frequencey（tf），Term在此文档中出现的频率，ft越大表示越重要
- Document Frequency（df），表示有多少文档中出现过这个Trem，df越大表示越不重要
  物以希为贵，大家都有的东西，自然就不那么贵重了，只有你专有的东西表示这个东西很珍贵，权重的公式：
  ![img](/images/全文检索/solr权重.png)

#### 空间向量模型

文档中词的权重看作一个向量

```
Document = {term1, term2, …… ,term N}
Document Vector = {weight1, weight2, …… ,weight N}
```

把欲要查询的语句看作一个简单的文档，也用向量表示：

```
Query = {term1, term 2, …… , term N}
Query Vector = {weight1, weight2, …… , weight N}
```

把搜索出的文档向量及查询向量放入N维度的空间中，每个词表示一维：
![img](/images/全文检索/solr空间向量模型.png)

夹角越小，表示越相似，相关性越大

### solr安装配置（7.7）

略

#### Jetty启动方式

```shell
cd /home/soft/solr/solr-7.7.0/bin/
# 启动solr
./solr start -force
```

```shell
# 启动solr自带的example
./solr start -force
```

创建solrcore和tomcat部署方式基本一样

```shell
cd /home/soft/solr/solr-7.7.0/server/solr
# 创建solrcore
mkdir lion_core
# 将配置文件复制到solrcore中
cp -r configsets/_default/conf/ lion_core/
```

5、启动jetty，访问http://192.168.1.63:8983/solr/，在Core Admin中新增core就可以了name自定义，instanceDir是创建的solrcore的文件夹名称，其他三个选项默认。

#### Tomcat部署方式

1、 解压后，将 solr-7.7.0\server\solr-webapp\webapp文件夹复制到Tomcat\webapps\目录下，并改成solr (名称随意,为了后面访问方便，改为solr）.

```shell
cp -r webapp /home/soft/tomcat/tomcat-solr/webapps/solr
```

2、新建一个文件夹：solrhome(保存solr core的) ，编辑/home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/web.xml文件。指定solrhome的位置，在web-app节点中加入以下代码：

```xml
<env-entry>
   <env-entry-name>solr/home</env-entry-name>
    <!--指定solrhome的位置-->
   <env-entry-value>/your/solr/home/path</env-entry-value>
   <env-entry-type>java.lang.String</env-entry-type>
</env-entry> 

```

并且注释掉一下代码，否则会出现403错误。

```xml
<!--把 security-constraint 整个标签注释：-->

<security-constraint>
    <web-resource-collection>
    <web-resource-name>Disable TRACE</web-resource-name>
    <url-pattern>/</url-pattern>
    <http-method>TRACE</http-method>
    </web-resource-collection>
    <auth-constraint/>
    </security-constraint>
    <security-constraint>
    <web-resource-collection>
    <web-resource-name>Enable everything but TRACE</web-resource-name>
    <url-pattern>/</url-pattern>
    <http-method-omission>TRACE</http-method-omission>
    </web-resource-collection>
</security-constraint>

```

3、将 `solr-7.7.0/server/lib/ext` 所有jar包( 会提示你是否覆盖disruptor-3.4.0.jar，确认即可)，以及`solr-7.7.0/server/lib` 下 metrics 相关的jar ，以及`solr-7.7.0/dist` 下 solr-dataimporthandler 相关的jar复制`/home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/lib`下

```shell
cp solr-7.7.0/server/lib/ext/* /home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/lib/
cp solr-7.7.0/server/lib/metrics-* /home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/lib/
cp solr-7.7.0/dist/solr-dataimporthandler-* /home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/lib/
```

 如果你想增加日志，那么复制server/resource/log4j2.xml  到classes 文件夹 

```shell
# 先创建classes目录
mkdir /home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/classes/
# 复制log4j.xml
cp server/resources/log4j2.xml /home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/classes/
```



4、创建solrcore

进入到solr_home目录，在该目录下创建一个solr_core文件夹，用于存储solr数据文件

```shell
cd /home/soft/solr/solr_home/
# 将解压文件中的所有配置复制到solrhome中（solr.xml等，否则报错）
cp -r /home/soft/solr/solr-7.7.0/server/solr/* ./
# 创建solrcore
mkdir lion_core
# 将配置文件复制到solrcore中
cp -r solr/configsets/_default/conf/ lion_core/
```

5、启动tomcat，访问http://192.168.1.63:8080/solr/index.html，在Core Admin中新增core就可以了name自定义，instanceDir是创建的solrcore的文件夹名称，其他三个选项默认。

#### 配置IK中文分析器

1、下载<https://github.com/magese/ik-analyzer-solr7> 
自己maven打包 成 ik-analyzer-solr7-7.x.jar 复制到`/home/soft/tomcat/tomcat-solr/webapp/solr/WEB-INF/lib`里面。 

2、在要添加IK分词器的solrcore中的conf/managed-schema文件中添加以下代码

```shell
vi lion_core/conf/managed-schema 
```

```xml
   <fieldType name="text_ik" class="solr.TextField">
        <analyzer type="index" useSmart="false" 
        class="org.wltea.analyzer.lucene.IKAnalyzer" />
        <analyzer type="query" useSmart="true"             class="org.wltea.analyzer.lucene.IKAnalyzer" />
    </fieldType>

```

#### dataimport数据库数据导入

`mysql-connector-java-5.1.42.jar` 放到 
`/home/soft/tomcat/tomcat-solr/webapps/solr/WEB-INF/lib/`

修改`solrconfig.xml`文件（在`/solrhome/solrcore/conf`下）

```xml
<requestHandler name="/dataimport" class="solr.DataImportHandler">
      <lst name="defaults">
            <str name="config">data-config.xml</str>
      </lst>
  </requestHandler>
```

创建`data-config.xml`（在`/solrhome/solrcore/conf`下创建）

```xml
<dataConfig>
    <!-- 这是mysql的配置，学会jdbc的都应该看得懂,注意&应该配置为&amp;-->
    <dataSource driver="com.mysql.jdbc.Driver" url="jdbc:mysql://localhost:3306/crm?useUnicode=true&amp;characterEncoding=utf-8;" user="root" password="root"/>
    <document>
        <!-- name属性，就代表着一个文档，可以随便命名 -->
        <!-- query是一条sql，代表在数据库查找出来的数据 -->
        <entity name="product" query="select * from products">
            <!-- 每一个field映射着数据库中列与文档中的域，column是数据库列，name是solr的域(必须是在`managed-schema`文件中配置过的域才行) -->
            <field column="pid" name="id"/>
            <field column="name" name="product_name"/>
            <field column="catalog" name="product_catalog"/>
            <field column="catalog_name" name="product_catalog_name"/>
            <field column="price" name="product_price"/>
            <field column="description" name="product_description"/>
            <field column="picture" name="product_picture"/>
        </entity>
    </document>
</dataConfig>
```

修改managed-schema（在`/solrhome/solrcore/conf`下创建）,添加如下内容：

```xml
<!-- 分词由type来控制，不同的type有不同的分词策略，indexed表示是否索引，stored表示是否存储
	其他的属性：required表示必须的，创建的时候该域必须存在。
			  mutivalue表示是否可以存储多个值，如果为true说明这是一个数组类型
	name属性需要与java实体类属性对应，否则需要@Field指定
-->
<field name="product_name" type="string" indexed="true" stored="true" />
<field name="product_catalog" type="string" indexed="true" stored="true" />
<field name="product_catalog_name" type="string" indexed="true" stored="true" />
<field name="product_price" type="string" indexed="true" stored="true" />
<field name="product_description" type="string" indexed="true" stored="true" />
<field name="product_picture" type="string" indexed="true" stored="true" />
```

>注：
>
>`managed-schema`文件其他标签说明
>
>1. <dynamicField>：动态域，一种模糊匹配的域。
>
>   > <dynamicField name="*_i" type ="int" indexed="true" stored="true"/>
>
>2. <copyField>：复制域，会把`cat`,`name`,`manu`三个域内容放到`yourField`中，这样只需要在`yourField`一个域中搜索就可以了，提高了搜索效率。
>
>   ><copyField source="cat" dest="yourField"/>
>   >
>   ><copyField source="name" dest="yourField"/>
>   >
>   ><copyField source="manu" dest="yourField"/>
>
>3. <fieldType>：配置分词
>
>   ><fieldType name="text_ik" class="solr.TextField">
>   >​        <analyzer type="index" useSmart="false" 
>   >​        class="org.wltea.analyzer.lucene.IKAnalyzer" />
>   >​        <analyzer type="query" useSmart="true"             class="org.wltea.analyzer.lucene.IKAnalyzer" />
>   >​    </fieldType>
>
>4. <uniqueKey>:唯一键
>
>   > solr会根据唯一键进行索引的覆盖操作，默认唯一键是id：
>   >
>   > 比如数据库数据发生更新，如果唯一键相同，会执行索引覆盖操作，否则是新增一条

#### 更新索引

全量或者增量更新索引需要修改`data-config.xml`（在`/solrhome/solrcore/conf`下）

##### 全量更新

默认的配置就可以实现全量更新的操作。

##### 增量更新

```xml
<!--  query:查询数据库表符合记录数据   ---> 
<!--  deltaQuery:增量索引查询主键ID    --->    注意这个只能返回ID字段 
<!--  deltaImportQuery:增量索引查询导入的数据  ---> 
<!--  deletedPkQuery:增量索引删除主键ID查询  ---> 注意这个只能返回ID字段
<!--  pk="id"：可以用来标识主键-->
<entity name="person" 
        query="select * from person" 
        deltaQuery="select * from person where name > '${dataimporter.last_index_time}'" 
        deltaImportQuery="select * from person where id='${dih.delta.id}'">
    <field column="id" name="id"/>
    <field column="name" name="name"/>
    <field column="description" name="person_description"/>
    <field column="birthday" name="person_birthday"/>
</entity>
```

> 意思是首先按照query指定的SQL语句查询出符合条件的记录。
>
> 然后从这些数据中根据deltaQuery指定的SQL语句查询出所有需要增量导入的数据的ID号。
>
> 最后根据deltaImportQuery指定的SQL语句返回所有这些ID的数据，即为这次增量导入所要处理的数据。

**核心思想是：通过内置变量“${dih.delta.id}”和 “${dataimporter.last_index_time}”来记录本次要索引的id和最近一次索引的时间。**

大家可以看到在上图中，entity标签里，query属性的select语句与deltaQuery属性的select语句是不一样的，差异在于多了一个`where modify_date > '${dataimporter.last_index_time}'`。

这是因为solr每次在全量导入时或者增量导入结束时，都会在`solrhome\conf\dataimport.properties`文件中为每一个entity更新一个导入时间标志，我本地截图如下：

![1551321876648](images\全文检索\solr索引更新时间.jpg)

这个配置文件里的属性对象可以在`data-config.xml`里以${}占位符方式填充数值。

重点1：所以在执行deltaQuery时，只会导入数据满足为：`modify_date > '${dataimporter.last_index_time}'`，以此实现增量导入。否则就是全量导入！！！

重点2：在deltaQuery语句中，select字段一定要加上你想更新到索引的字段，否则默认不导入为索引！！

**注意：**

增量更新需要写两个sql的配置（deltaImportQuery、deltaQuery）

```shell
deltaImportQuery="select * where id='${dih.delta.id}'"
deltaQuery="select id from book_dept where '列名' > '${dih.last_index_time}'"
```

### SolrCloud集群

* 当需要大规模，容错，分布式索引和检索能力时使用SolrCloud。
* 当索引量很大（10G），搜索请求并发很高时，同样需要使用SolrCloud来满足需求。
* 不过当一个系统的索引数据量少的时候是不需要使用SolrCloud的。

### SolrCloud架构图

####1、索引（collection）的逻辑图

![img](/images/全文检索/索引的逻辑图.png)

#### 2、Solr和索引对照图

![img](/images/全文检索/solrcloud应用架构)



####3、 创建索引过程

![img](/images/全文检索/solrcloud创建索引过程.png)

 ####4、分布式查询

![img](C:\Users\Ninee\Desktop\notes\images\全文检索\solrcloud分布式查询.png)



####5、Shard Splitting

![img](/images/全文检索/solr cloud Shard Splitting.png)

### SolrCloud集群搭建

####什么是SolrCloud

 SolrCloud(solr 云)是Solr提供的分布式搜索方案，当你需要大规模，容错，分布式索引和检索能力时使用SolrCloud。当一个系统的索引数据量少的时候是不需要使用SolrCloud的，当索引量很大，搜索请求并发很高，这时需要使用SolrCloud来满足这些需求。

SolrCloud是基于Solr和Zookeeper的分布式搜索方案，它的主要思想是使用Zookeeper作为集群的配置信息中心。

它有几个特色功能：

**1）集中式的配置信息**

**2）自动容错**

**3）近实时搜索**

**4）查询时自动负载均衡**

#### 首先需要搭建Zookeeper集群

> 参照文件ZooKeeper.md

#### SolrCloud搭建

**注：SolrCloud启动之前，需要先搭建启动ZooKeeper集群。**

需要4台机器搭建Solr服务

##### 第一步：复制单机版Solr服务对应的Tomcat，并修改端口为8888

搭建好了单机版直接复制就可以了，没搭建好就先搭建单机版就可以了

##### 第二步：复制solrhome，改名solrhome8888，一个solrcore对应一个solrhome

##### 第三步：修改每个Solr服务的web.xml,分别指定对应的solrhome路径

```xml
<env-entry>
   <env-entry-name>solr/home</env-entry-name>
    <!--指定solrhome的位置-->
   <env-entry-value>/your/solr/home/path</env-entry-value>
   <env-entry-type>java.lang.String</env-entry-type>
</env-entry> 
```

##### 第四步：修改每个solrhome下的solr.xml，指定对应solr服务的Tomcat的ip和端口

修改SolrCloud监控端口，将host改成虚拟机ip地址，port改成对应的tomcat的端口号。

```xml
<solr>
	<solrcloud>
    	<str name="host">${host:192.168.1.63}</str>
        <int name="hostPort">${jetty.port:8083}</int>
        <str name="hostContext">${hostContext:solr}</str>
        <int name="zkClientTimeout">${zkClientTimeout:30000}</int>
        <bool name="genericCoreNodeNames">${genericCoreNodeNames:true}</bool>
    </solrcloud>
</solr>
```



##### 第五步：每一台solr和zookeeper关联

设置tomcat的启动参数，再Tomcat目录下的bin/catalina.sh添加以下内容

```shell
# 指定zookeeper的地址
JAVA_OPTS="-DzkHost=192.168.1.63:2181,192.168.1.64:2181,192.168.1.65:2181"
```



##### 第六步：将solr的配置文件上传到zookeeper进行统一管理

由于zookeeper统一管理solr的配置文件（主要是schema.xml、solrconfig.xml）， solrCloud各各节点使用zookeeper管理的配置文件。

`zkcli.sh`在`solr-7.7.0/example/scripts/cloud-scripts/`目录下(solr解压目录)：

```shell
# 将solrcore中的配置文件上传到zookeeper
# -zkhost指定zookeeper集群位置；-cmd请求指令；-confdir指定要上传的文件；-confname指定上传后的重命名
./zkcli.sh -zkhost 192.168.1.63:2181,192.168.1.64:2181,192.168.1.65:2181 -cmd upconfig -confdir /home/soft/solrhome/my_core/conf -confname myconf
```

**注：这个命令只需要在一台机器上执行就可以了**



##### 第七步：启动solr

启动每一台solr的tomcat服务。

##### 第八步：访问solrcloud

访问任意一台solr，左侧菜单出现Cloud（这里的图是网上找的，所以IP不对应）：

![img](images\全文检索\solrcloud)

#### SolrCloud配置

##### 添加shared分片

上图中的collection1集群只有一片，可以通过下边的方法配置新的集群。

如果集群中有四个solr节点创建新集群collection2，将集群分为两片，每片两个副本。

访问以下链接（任意一台solr机器）发送Http请求即可：

```tex
http://192.168.1.63:8080/solr/admin/collections?action=CREATE&name=my_core&numShards=2&replicationFactor=2
```

![img](images\全文检索\solrcloud增加shared)

##### 删除shared分片

```tex
http://192.168.25.154:8080/solr/admin/collections?action=DELETE&name=collection1
```

更多的命令请参数官方文档：`apache-solr-ref-guide-4.10.pdf`





推荐文章：

https://blog.csdn.net/weixin_38970805/article/details/83144036

官方文档:

http://lucene.apache.org/solr/guide/8_1/solr-tutorial.html

## ElasticeSearch

## Solr和ElasticSearch对比



## 其他

使用solr时，不要把所有索引放到一个core中，会遇到很多麻烦，笔者亲身试坑，说多了都是泪。。